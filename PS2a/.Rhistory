#rm(list = ls())
library(pacman)
p_load("foreign","dplyr","magrittr","knitr","ggplot2", "corrplot", "stargazer", "glmnet","splines", "sandwich", "lmtest", "plm", "lfe")
theme_plot <- theme(
legend.position = "right",
panel.background = element_rect(fill = NA),
panel.border = element_rect(fill = NA, color = "grey75"),
axis.ticks = element_line(color = "grey85"),
panel.grid.major = element_line(color = "grey95", size = 0.2),
panel.grid.minor = element_line(color = "grey95", size = 0.2),
legend.key = element_blank(),
legend.title = element_blank(),
legend.spacing.x = unit(0.3, "cm"))
setwd("~/Dropbox/Berkeley_tings/Fall 2018/ARE213/Problem Sets/SharedFiles/are213/PS2a")
#setwd("C:\\Users\\will-\\Desktop\\are213\\PS1b")
#setwd("C:\\Users\\Will\\Desktop\\are213\\PS2a")
dat <- read.dta("traffic_safety2.dta")
setwd("~/Dropbox/Berkeley_tings/Fall 2018/ARE213/Problem Sets/SharedFiles/are213/PS2a")
#setwd("C:\\Users\\will-\\Desktop\\are213\\PS1b")
#setwd("C:\\Users\\Will\\Desktop\\are213\\PS2a")
dat <- read.dta("traffic_safety2.dta")
setwd("~/Dropbox/Berkeley_tings/Fall 2018/ARE213/Problem Sets/SharedFiles/are213/PS2a")
#setwd("C:\\Users\\will-\\Desktop\\are213\\PS1b")
#setwd("C:\\Users\\Will\\Desktop\\are213\\PS2a")
dat <- read.dta("traffic_safety2.dta")
# as.factor command on fixed effects
dat$state <- as.factor(dat$state)
dat$year <- as.factor(dat$year)
dat$fatal_pc <- log(dat$fatalities/dat$population)
# fix the attribute labels
attributes(dat)$var.labels = c("state", "year", attributes(dat)$var.labels[-(1:2)],
"log of traffic fatalities per capita")
# pooled bivariate OLS
reg1 <- lm(fatal_pc ~ primary, data = dat)
summary(reg1)
dat$time <- as.numeric(as.character(dat$year))-1981
dat$time_sq <- (dat$time)^2
# pooled bivariate OLS
reg2 <- lm(fatal_pc ~ primary + dat$time_sq, data = dat)
summary(reg2)
# pooled bivariate OLS
reg3 <- lm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip +
snow32 + rural_speed + urban_speed, data = dat)
summary(reg3)
#OLS coefficients and regular standard errors
round(coeftest(reg3),4)
#OLS coefficients and white standard errors
round(coeftest(reg3, vcov = vcovHC(reg3, type = "HC1")),4)
formula <- 'fatal_pc ~ primary + college + beer + primary + secondary + population + unemploy +
totalvmt + precip + snow32 + rural_speed + urban_speed'
felm_formula_clust <- paste(formula,'| 0 | 0 | state', sep = '') %>%
as.formula()
felm.clust <- felm( felm_formula_clust,
dat)
# the estimates don't change, but the standard errors do
#OLS coefficients and regular standard errors
round(coeftest(felm.clust),4)
# Function to convert tibble, data.frame, or tbl_df to matrix
to_matrix <- function(the_df, vars) {
# Create a matrix from variables in var
new_mat <- the_df %>%
# Select the columns given in 'vars'
select_(.dots = vars) %>%
# Convert to matrix
as.matrix()
# Return 'new_mat'
return(new_mat)
}
#OLS
b_ols <- function(y, X) {
# Calculate beta hat
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
# Return beta_hat
return(beta_hat)
}
#create function to calculate vcov matrix
vcov_cluster <- function(data, y_var, X_vars,
cluster_var, intercept = T) {
# Turn data into matrices
y <- to_matrix(data, y_var)
X <- to_matrix(data, X_vars)
# Add intercept
if (intercept == T) X <- cbind(1, X)
# Calculate n and k for degrees of freedom
n <- nrow(X)
k <- ncol(X)
# Estimate coefficients
b <- b_ols(y, X)
# Update names
if (intercept == T) rownames(b)[1] <- "Intercept"
# Calculate OLS residuals
e <- y - X %*% b
# Inverse of X'X
XX_inv <- solve(t(X) %*% X)
# Find the levels of the variable on which we are clustering
cl_levels <- data[, cluster_var] %>% unique() %>% unlist()
# Calculate the meat, iterating over the clusters
meat_hat <- lapply(X = cl_levels, FUN = function(g) {
# Find the row indices for the current cluster
indices <- which(unlist(data[, cluster_var]) == g)
# Grab the current cluster's rows from X and e
X_g <- X[indices,]
e_g <- e[indices] %>% matrix(ncol = 1)
# Calculate this cluster's part of the meat estimate
return(t(X_g) %*% e_g %*% t(e_g) %*% X_g)
}) %>% Reduce(f = "+", x = .) / n
# Find the number of clusters
G <- length(cl_levels)
# Degrees-of-freedom correction
df_c <- G/(G-1) * (n-1)/(n-k)
# Return the results
return(df_c * n * XX_inv %*% meat_hat %*% XX_inv)
}
# get the vcov matrix
y <- 'fatal_pc'
x <- c('primary','college', 'beer', 'secondary' , 'population' , 'unemploy' , 'totalvmt' , 'precip' ,
'snow32' , 'rural_speed' ,'urban_speed')
clus <- 'state'
vcov.lm.clust = vcov_cluster(dat,y,x,clus)
# show results
round(coeftest(reg3, vcov = vcov.lm.clust),4)
between_nocov <- plm(fatal_pc ~ primary, data = dat, model = "between")
summary(between_nocov)
between_cov <- plm(fatal_pc ~ primary + college + beer + secondary + population + unemploy +
totalvmt + precip + snow32 + rural_speed + urban_speed,
data = dat, model = "between")
summary(between_cov)
rand_eff <- plm(fatal_pc ~ primary + college + beer + secondary + population +
unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed,
data = dat, index =c("state","year"),model = "random")
summary(rand_eff)
C <- length(unique(dat$state))
N <- length(dat$state)
K <- 11 +                               # all the other covariates
1 +                                # the intercept
length(unique(dat$state)) - 1 +   # the state dummies - ommitted state = 1
length(unique(dat$year)) - 1      # the year dummies - ommitted year = 1981
adjustment <- (C/(C - 1)) * (N - 1)/(N - K)
remove(C, N, K)
# when we cluster within group, we need to use the arellano method
#   when we cluster within time, we need to use the white method (see help file)
vcov.plm.clust = vcovHC(rand_eff,
method = "arellano",
cluster = "group") * adjustment
# save results
round(coeftest(rand_eff, vcov = vcov.plm.clust),4)
#non-clustered results
formula <- 'fatal_pc ~ primary + time_sq'
fixed_eff <- felm( as.formula( paste( formula, '+ year | state | 0 | 0' ) ),
dat)
summary(fixed_eff)
#non-clustered results
formula <- 'fatal_pc ~ primary + time_sq'
fixed_eff <- felm( as.formula( paste( formula, '+ year | state | 0 | state' ) ),
dat)
summary(fixed_eff)
formula <- 'fatal_pc ~ primary + college + beer + primary + secondary + population + unemploy +
totalvmt + precip + snow32 + rural_speed + urban_speed'
fixed_eff <- felm( as.formula( paste( formula, '+ year | state | 0 | 0' ) ),
dat)
summary(fixed_eff)
