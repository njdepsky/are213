clustered(lm.noclust, dat$state)
formula <- 'fatal_pc ~ primary + year + college + beer + primary + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed'
felm_formula_clust <- paste(formula,'| 0 | 0 | state', sep = '') %>%
as.formula()
felm.clust <- felm( felm_formula_clust,
dat)
# the estimates don't change, but the standard errors do
#OLS coefficients and regular standard errors
round(coeftest(felm.clust),4)
#Manually calculating clustered se that are robust to within-state corr
lm.noclust = lm(formula, dat)
clustered <- function(model, cluster){
M <- length(unique(cluster))
N <- length(cluster)
K <- model$rank
dfc <- (M/(M-1))*((N-1)/(N-K))
uj  <- apply(sandwich::estfun(model), 2, function(x) tapply(x, cluster, sum));
vcovCL <- dfc * sandwich::sandwich(model, meat=crossprod(uj)/N)
return(lmtest::coeftest(model, vcovCL))
}
round(clustered(lm.noclust, dat$state),4)
View(dat)
formula <- 'fatal_pc ~ primary + year + college + beer + primary + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed'
felm_formula_clust <- paste(formula,'| 0 | 0 | state', sep = '') %>%
as.formula()
felm.clust <- felm( felm_formula_clust,
dat)
# the estimates don't change, but the standard errors do
#OLS coefficients and regular standard errors
round(coeftest(felm.clust),4)
formula <- 'fatal_pc ~ primary + college + beer + primary + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed'
felm_formula_clust <- paste(formula,'| 0 | 0 | state', sep = '') %>%
as.formula()
felm.clust <- felm( felm_formula_clust,
dat)
# the estimates don't change, but the standard errors do
#OLS coefficients and regular standard errors
round(coeftest(felm.clust),4)
# pooled bivariate OLS
reg3 <- lm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed, data = dat)
summary(reg3)
#OLS coefficients and regular standard errors
round(coeftest(reg3),4)
#OLS coefficients and white standard errors
round(coeftest(reg3, vcov = vcovHC(reg3, type = "HC1")),4)
formula <- 'fatal_pc ~ primary + college + beer + primary + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed'
felm_formula_clust <- paste(formula,'| 0 | 0 | state', sep = '') %>%
as.formula()
felm.clust <- felm( felm_formula_clust,
dat)
# the estimates don't change, but the standard errors do
#OLS coefficients and regular standard errors
round(coeftest(felm.clust),4)
# pooled bivariate OLS
reg3 <- lm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed, data = dat)
round(summary(reg3),4)
# pooled bivariate OLS
reg3 <- lm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed, data = dat)
summary(reg3)
#Manually calculating clustered se that are robust to within-state corr (using lm)
lm.noclust = lm(as.formula( paste(formula)),
model = FALSE, x = TRUE, dat)
X = lm.noclust$x
# list of all the states, we need to un-factor this (it's why factors are tricky)
states = unique(as.numeric(as.character(dat$state)))
# V.hat estimate of V, solve returns the inverse of a matrix
V.hat = solve(t(X) %*% X)
# initialize Omega.hat, the estimate of Omega
Omega.hat = 0
# for each state we're going to calculate Xs' (es)(es') Xs and add it to Omega.hat
for(s in states){
if(s == 1){
boulean.s = c(rep(TRUE, length(unique(data$year))),
rep(FALSE, dim(X)[1] - length(unique(data$year))) )
}else{
boulean.s = (X[,paste('state',s, sep = '')] == 1 )
}
X.s = X[ boulean.s, ]
e.s = lm.noclust$residuals[boulean.s]
Omega.hat = Omega.hat + t(X.s) %*% (e.s%*% t(e.s)) %*% X.s
}
#Manually calculating clustered se that are robust to within-state corr (using lm)
lm.noclust = lm(as.formula( paste(formula)),
model = FALSE, x = TRUE, dat)
X = lm.noclust$x
# list of all the states, we need to un-factor this (it's why factors are tricky)
states = unique(as.numeric(as.character(dat$state)))
# V.hat estimate of V, solve returns the inverse of a matrix
V.hat = solve(t(X) %*% X)
# initialize Omega.hat, the estimate of Omega
Omega.hat = 0
# for each state we're going to calculate Xs' (es)(es') Xs and add it to Omega.hat
for(s in states){
if(s == 1){
boulean.s = c(rep(TRUE, length(unique(dat$year))),
rep(FALSE, dim(X)[1] - length(unique(dat$year))) )
}else{
boulean.s = (X[,paste('state',s, sep = '')] == 1 )
}
X.s = X[ boulean.s, ]
e.s = lm.noclust$residuals[boulean.s]
Omega.hat = Omega.hat + t(X.s) %*% (e.s%*% t(e.s)) %*% X.s
}
s in states
remove(X,  states, s, boulean.s, X.s, e.s)
#Manually calculating clustered se that are robust to within-state corr (using lm)
lm.noclust = lm(as.formula( paste(formula)),
model = FALSE, x = TRUE, dat)
X = lm.noclust$x
# list of all the states, we need to un-factor this (it's why factors are tricky)
states = unique(as.numeric(as.character(dat$state)))
# V.hat estimate of V, solve returns the inverse of a matrix
V.hat = solve(t(X) %*% X)
# initialize Omega.hat, the estimate of Omega
Omega.hat = 0
# for each state we're going to calculate Xs' (es)(es') Xs and add it to Omega.hat
for(s in states){
if(s == 1){
boulean.s = c(rep(TRUE, length(unique(dat$year))),
rep(FALSE, dim(X)[1] - length(unique(dat$year))) )
}else{
boulean.s = (X[,paste('state',s, sep = '')] == 1 )
}
X.s = X[ boulean.s, ]
e.s = lm.noclust$residuals[boulean.s]
Omega.hat = Omega.hat + t(X.s) %*% (e.s%*% t(e.s)) %*% X.s
}
#Manually calculating clustered se that are robust to within-state corr (using lm)
lm.noclust = lm(as.formula( paste(formula)),
model = FALSE, x = TRUE, dat)
X = lm.noclust$x
# list of all the states, we need to un-factor this (it's why factors are tricky)
states = unique(as.numeric(as.character(dat$state)))
# V.hat estimate of V, solve returns the inverse of a matrix
V.hat = solve(t(X) %*% X)
# initialize Omega.hat, the estimate of Omega
Omega.hat = 0
# for each state we're going to calculate Xs' (es)(es') Xs and add it to Omega.hat
for(s in states){
if(s == 1){
boulean.s = c(rep(TRUE, length(unique(dat$year))),
rep(FALSE, dim(X)[1] - length(unique(dat$year))) )
}else{
boulean.s = (X[,paste('state',s, sep = '')] == 1 )
}
X.s = X[ boulean.s, ]
e.s = lm.noclust$residuals[boulean.s]
Omega.hat = Omega.hat + t(X.s) %*% (e.s%*% t(e.s)) %*% X.s
}
remove(X,  states, s, boulean.s, X.s, e.s)
View(Omega.hat)
# list of all the states, we need to un-factor this (it's why factors are tricky)
states = unique(as.numeric(as.character(dat$state)))
X = lm.noclust$x
View(X)
formula <- 'fatal_pc ~ primary + state + college + beer + primary + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed'
#Manually calculating clustered se that are robust to within-state corr (using lm)
lm.noclust = lm(as.formula( paste(formula)),
model = FALSE, x = TRUE, dat)
X = lm.noclust$x
# list of all the states, we need to un-factor this (it's why factors are tricky)
states = unique(as.numeric(as.character(dat$state)))
# V.hat estimate of V, solve returns the inverse of a matrix
V.hat = solve(t(X) %*% X)
# initialize Omega.hat, the estimate of Omega
Omega.hat = 0
# for each state we're going to calculate Xs' (es)(es') Xs and add it to Omega.hat
for(s in states){
if(s == 1){
boulean.s = c(rep(TRUE, length(unique(dat$year))),
rep(FALSE, dim(X)[1] - length(unique(dat$year))) )
}else{
boulean.s = (X[,paste('state',s, sep = '')] == 1 )
}
X.s = X[ boulean.s, ]
e.s = lm.noclust$residuals[boulean.s]
Omega.hat = Omega.hat + t(X.s) %*% (e.s%*% t(e.s)) %*% X.s
}
# Function to convert tibble, data.frame, or tbl_df to matrix
to_matrix <- function(the_df, vars) {
# Create a matrix from variables in var
new_mat <- the_df %>%
# Select the columns given in 'vars'
select_(.dots = vars) %>%
# Convert to matrix
as.matrix()
# Return 'new_mat'
return(new_mat)
}
#OLS
b_ols <- function(y, X) {
# Calculate beta hat
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
# Return beta_hat
return(beta_hat)
}
#create function to calculate vcov matrix
vcov_cluster <- function(data, y_var, X_vars,
cluster_var, intercept = T) {
# Turn data into matrices
y <- to_matrix(data, y_var)
X <- to_matrix(data, X_vars)
# Add intercept
if (intercept == T) X <- cbind(1, X)
# Calculate n and k for degrees of freedom
n <- nrow(X)
k <- ncol(X)
# Estimate coefficients
b <- b_ols(y, X)
# Update names
if (intercept == T) rownames(b)[1] <- "Intercept"
# Calculate OLS residuals
e <- y - X %*% b
# Inverse of X'X
XX_inv <- solve(t(X) %*% X)
# Find the levels of the variable on which we are clustering
cl_levels <- data[, cluster_var] %>% unique() %>% unlist()
# Calculate the meat, iterating over the clusters
meat_hat <- lapply(X = cl_levels, FUN = function(g) {
# Find the row indices for the current cluster
indices <- which(unlist(data[, cluster_var]) == g)
# Grab the current cluster's rows from X and e
X_g <- X[indices,]
e_g <- e[indices] %>% matrix(ncol = 1)
# Calculate this cluster's part of the meat estimate
return(t(X_g) %*% e_g %*% t(e_g) %*% X_g)
}) %>% Reduce(f = "+", x = .) / n
# Find the number of clusters
G <- length(cl_levels)
# Degrees-of-freedom correction
df_c <- G/(G-1) * (n-1)/(n-k)
# Return the results
return(df_c * n * XX_inv %*% meat_hat %*% XX_inv)
}
#Manually calculating clustered se that are robust to within-state corr (using lm)
lm.noclust = lm(as.formula(paste(formula), dat))
#Manually calculating clustered se that are robust to within-state corr (using lm)
lm.noclust = lm(as.formula(paste(formula), dat))
#Manually calculating clustered se that are robust to within-state corr (using lm)
lm.noclust <- lm(as.formula(paste(formula), dat))
# get the vcov matrix
y <- 'fatal_pc'
x <- 'primary + college + beer + primary + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed'
clus <- 'state'
vcov.lm.clust = vcov_cluster(dat,y,x,clus)
# get the vcov matrix
y <- 'fatal_pc'
x <- c('primary','college', 'beer', 'secondary' , 'population' , 'unemploy' , 'totalvmt' , 'precip' , 'snow32' , 'rural_speed' ,'urban_speed')
clus <- 'state'
vcov.lm.clust = vcov_cluster(dat,y,x,clus)
# show results
round(coeftest(reg3, vcov = vcov.lm.clust),4)
between_nocov <- plm(fatal_pc ~ primary, data = dat, model = "between")
summary(between_nocov)
between_cov <- plm(fat_pc ~ fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed,data = dat, model = "between")
between_cov <- plm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed, data = dat, model = "between")
summary(between_cov)
rand_eff <- plm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed, data = dat, model = "random")
summary(between_cov)
rand_eff <- plm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed, data = dat, model = "random")
summary(rand_eff)
names(attributes(dat))
attributes(dat)$var.labels
# as.factor command on fixed effects
dat$state <- as.factor(data$state)
# as.factor command on fixed effects
dat$state <- as.factor(dat$state)
dat$year <- as.factor(dat$year)
dat$fatal_pc <- log(dat$fatalities/dat$population)
# fix the attribute labels
attributes(dat)$var.labels = c("state", "year", attributes(dat)$var.labels[-(1:2)], "log of traffic fatalities per capita")
# pooled bivariate OLS
reg1 <- lm(fatal_pc ~ primary, data = dat)
summary(reg1)
rand_eff <- plm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed + state, data = dat, model = "random")
View(dat)
rand_eff <- plm(fatal_pc ~ primary + state, data = dat, model = "random")
rand_eff <- plm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed, data = dat, index =c("state"),model = "random")
summary(rand_eff)
rand_eff <- plm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed, data = dat, index =c("state"),model = "within")
summary(rand_eff)
rand_eff <- plm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed, data = dat, index =c("state"),model = "random")
summary(rand_eff)
rand_eff <- plm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed, data = dat, index =c("state","year"),model = "random")
summary(rand_eff)
length(unique(dat$state))
K <- 11 +                               # all the other covariates
1 +                                # the intercept
length(unique(dat$state)) - 1 +   # the state dummies - ommitted state = 1
length(unique(dat$year)) - 1
C <- length(unique(dat$state))
N <- length(dat$state)
K <- 11 +                               # all the other covariates
1 +                                # the intercept
length(unique(dat$state)) - 1 +   # the state dummies - ommitted state = 1
length(unique(dat$year)) - 1      # the year dummies - ommitted year = 1981
adjustment <- (C/(C - 1)) * (N - 1)/(N - K)
remove(C, N, K)
# when we cluster within group, we need to use the arellano method
#   when we cluster within time, we need to use the white method (see help file)
vcov.plm.clust = vcovHC(rand_eff,
method = "arellano",
cluster = "group") * adjustment
# save results
round(coeftest(rand_eff, vcov = vcov.plm.clust),4)
rand_eff <- plm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed, data = dat, index =c("state","year"),model = "random")
round(summary(rand_eff),4
rand_eff <- plm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed, data = dat, index =c("state","year"),model = "random")
round(summary(rand_eff),4)
rand_eff <- plm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed, data = dat, index =c("state","year"),model = "random")
summary(rand_eff)
formula
formula <- 'fatal_pc ~ primary + college + beer + primary + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed'
felm_formula_clust <- paste(formula,'| 0 | 0 | state', sep = '') %>%
as.formula()
felm.clust <- felm( felm_formula_clust,
dat)
# the estimates don't change, but the standard errors do
#OLS coefficients and regular standard errors
round(coeftest(felm.clust),4)
fixed_eff <- felm( as.formula( paste( formula, '+ year | state | 0 | 0' ) ),
dat)
summary(fixed_eff)
formula <- 'fatal_pc ~ primary + time_sq'
fixed_eff <- felm( as.formula( paste( formula, '+ year | state | 0 | 0' ) ),
dat)
summary(fixed_eff)
formula <- 'fatal_pc ~ primary + college + beer + primary + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed'
fixed_eff <- felm( as.formula( paste( formula, '+ year | state | 0 | 0' ) ),
dat)
summary(fixed_eff)
View(dat)
#non-clustered results
formula <- 'fatal_pc ~ primary + time_sq'
fixed_eff <- felm( as.formula( paste( formula, '+ year | state | 0 | state' ) ),
dat)
summary(fixed_eff)
#rm(list = ls())
library(pacman)
p_load("foreign","dplyr","magrittr","knitr","ggplot2", "corrplot", "stargazer", "glmnet","splines", "sandwich", "lmtest", "plm", "lfe")
theme_plot <- theme(
legend.position = "right",
panel.background = element_rect(fill = NA),
panel.border = element_rect(fill = NA, color = "grey75"),
axis.ticks = element_line(color = "grey85"),
panel.grid.major = element_line(color = "grey95", size = 0.2),
panel.grid.minor = element_line(color = "grey95", size = 0.2),
legend.key = element_blank(),
legend.title = element_blank(),
legend.spacing.x = unit(0.3, "cm"))
#setwd("~/Dropbox/Berkeley_tings/Fall 2018/ARE213/Problem Sets/SharedFiles/are213/PS1b")
#setwd("C:\\Users\\will-\\Desktop\\are213\\PS1b")
setwd("C:\\Users\\Will\\Desktop\\are213\\PS2a")
dat <- read.dta("traffic_safety2.dta")
# as.factor command on fixed effects
dat$state <- as.factor(dat$state)
dat$year <- as.factor(dat$year)
dat$fatal_pc <- log(dat$fatalities/dat$population)
# fix the attribute labels
attributes(dat)$var.labels = c("state", "year", attributes(dat)$var.labels[-(1:2)], "log of traffic fatalities per capita")
# pooled bivariate OLS
reg1 <- lm(fatal_pc ~ primary, data = dat)
summary(reg1)
dat$time_sq <- dat$year^2
# pooled bivariate OLS
reg2 <- lm(fatal_pc ~ primary + dat$year + dat$time_sq, data = dat)
dat$time <- as.numeric(as.character(dat$year))-1981
dat$time_sq <- (dat$time)^2
#rm(list = ls())
library(pacman)
p_load("foreign","dplyr","magrittr","knitr","ggplot2", "corrplot", "stargazer", "glmnet","splines", "sandwich", "lmtest", "plm", "lfe")
theme_plot <- theme(
legend.position = "right",
panel.background = element_rect(fill = NA),
panel.border = element_rect(fill = NA, color = "grey75"),
axis.ticks = element_line(color = "grey85"),
panel.grid.major = element_line(color = "grey95", size = 0.2),
panel.grid.minor = element_line(color = "grey95", size = 0.2),
legend.key = element_blank(),
legend.title = element_blank(),
legend.spacing.x = unit(0.3, "cm"))
#setwd("~/Dropbox/Berkeley_tings/Fall 2018/ARE213/Problem Sets/SharedFiles/are213/PS1b")
#setwd("C:\\Users\\will-\\Desktop\\are213\\PS1b")
setwd("C:\\Users\\Will\\Desktop\\are213\\PS2a")
dat <- read.dta("traffic_safety2.dta")
# as.factor command on fixed effects
dat$state <- as.factor(dat$state)
dat$year <- as.factor(dat$year)
dat$fatal_pc <- log(dat$fatalities/dat$population)
# fix the attribute labels
attributes(dat)$var.labels = c("state", "year", attributes(dat)$var.labels[-(1:2)], "log of traffic fatalities per capita")
# pooled bivariate OLS
reg1 <- lm(fatal_pc ~ primary, data = dat)
summary(reg1)
dat$time <- as.numeric(as.character(dat$year))-1981
dat$time_sq <- (dat$time)^2
# pooled bivariate OLS
reg2 <- lm(fatal_pc ~ primary + dat$time_sq, data = dat)
summary(reg2)
# pooled bivariate OLS
reg3 <- lm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed, data = dat)
summary(reg3)
#OLS coefficients and regular standard errors
round(coeftest(reg3),4)
#OLS coefficients and white standard errors
round(coeftest(reg3, vcov = vcovHC(reg3, type = "HC1")),4)
formula <- 'fatal_pc ~ primary + college + beer + primary + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed'
felm_formula_clust <- paste(formula,'| 0 | 0 | state', sep = '') %>%
as.formula()
felm.clust <- felm( felm_formula_clust,
dat)
# the estimates don't change, but the standard errors do
#OLS coefficients and regular standard errors
round(coeftest(felm.clust),4)
# Function to convert tibble, data.frame, or tbl_df to matrix
to_matrix <- function(the_df, vars) {
# Create a matrix from variables in var
new_mat <- the_df %>%
# Select the columns given in 'vars'
select_(.dots = vars) %>%
# Convert to matrix
as.matrix()
# Return 'new_mat'
return(new_mat)
}
#OLS
b_ols <- function(y, X) {
# Calculate beta hat
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
# Return beta_hat
return(beta_hat)
}
#create function to calculate vcov matrix
vcov_cluster <- function(data, y_var, X_vars,
cluster_var, intercept = T) {
# Turn data into matrices
y <- to_matrix(data, y_var)
X <- to_matrix(data, X_vars)
# Add intercept
if (intercept == T) X <- cbind(1, X)
# Calculate n and k for degrees of freedom
n <- nrow(X)
k <- ncol(X)
# Estimate coefficients
b <- b_ols(y, X)
# Update names
if (intercept == T) rownames(b)[1] <- "Intercept"
# Calculate OLS residuals
e <- y - X %*% b
# Inverse of X'X
XX_inv <- solve(t(X) %*% X)
# Find the levels of the variable on which we are clustering
cl_levels <- data[, cluster_var] %>% unique() %>% unlist()
# Calculate the meat, iterating over the clusters
meat_hat <- lapply(X = cl_levels, FUN = function(g) {
# Find the row indices for the current cluster
indices <- which(unlist(data[, cluster_var]) == g)
# Grab the current cluster's rows from X and e
X_g <- X[indices,]
e_g <- e[indices] %>% matrix(ncol = 1)
# Calculate this cluster's part of the meat estimate
return(t(X_g) %*% e_g %*% t(e_g) %*% X_g)
}) %>% Reduce(f = "+", x = .) / n
# Find the number of clusters
G <- length(cl_levels)
# Degrees-of-freedom correction
df_c <- G/(G-1) * (n-1)/(n-k)
# Return the results
return(df_c * n * XX_inv %*% meat_hat %*% XX_inv)
}
# get the vcov matrix
y <- 'fatal_pc'
x <- c('primary','college', 'beer', 'secondary' , 'population' , 'unemploy' , 'totalvmt' , 'precip' , 'snow32' , 'rural_speed' ,'urban_speed')
clus <- 'state'
vcov.lm.clust = vcov_cluster(dat,y,x,clus)
# show results
round(coeftest(reg3, vcov = vcov.lm.clust),4)
between_nocov <- plm(fatal_pc ~ primary, data = dat, model = "between")
summary(between_nocov)
between_cov <- plm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed, data = dat, model = "between")
summary(between_cov)
rand_eff <- plm(fatal_pc ~ primary + college + beer + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed, data = dat, index =c("state","year"),model = "random")
summary(rand_eff)
C <- length(unique(dat$state))
N <- length(dat$state)
K <- 11 +                               # all the other covariates
1 +                                # the intercept
length(unique(dat$state)) - 1 +   # the state dummies - ommitted state = 1
length(unique(dat$year)) - 1      # the year dummies - ommitted year = 1981
adjustment <- (C/(C - 1)) * (N - 1)/(N - K)
remove(C, N, K)
# when we cluster within group, we need to use the arellano method
#   when we cluster within time, we need to use the white method (see help file)
vcov.plm.clust = vcovHC(rand_eff,
method = "arellano",
cluster = "group") * adjustment
# save results
round(coeftest(rand_eff, vcov = vcov.plm.clust),4)
#non-clustered results
formula <- 'fatal_pc ~ primary + time_sq'
fixed_eff <- felm( as.formula( paste( formula, '+ year | state | 0 | 0' ) ),
dat)
summary(fixed_eff)
#non-clustered results
formula <- 'fatal_pc ~ primary + time_sq'
fixed_eff <- felm( as.formula( paste( formula, '+ year | state | 0 | state' ) ),
dat)
summary(fixed_eff)
formula <- 'fatal_pc ~ primary + college + beer + primary + secondary + population + unemploy + totalvmt + precip + snow32 + rural_speed + urban_speed'
fixed_eff <- felm( as.formula( paste( formula, '+ year | state | 0 | 0' ) ),
dat)
summary(fixed_eff)
